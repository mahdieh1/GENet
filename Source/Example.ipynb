# Step 1: Clone the repository
!git clone https://github.com/mahdieh1/GENet.git
%cd GENet

# Step 2: Install dependencies
!pip install numpy pandas seaborn matplotlib scipy scikit-learn torch torch-geometric

# Step 3: Define the model and functions

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import torch
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from sklearn.linear_model import LinearRegression
import torch.nn as nn
import torch.optim as optim
import seaborn as sns
import matplotlib.pyplot as plt

# Common names (cell lines) to be used as rows in our matrices
common_names = [
    "ENCFF603FVA", "ENCFF810ENN", "ENCFF150UES", "ENCFF787TTT", "ENCFF314FVV",
    "ENCFF480ZIG", "ENCFF389RXK", "ENCFF175UON", "ENCFF869HPN", "ENCFF894ESL",
    "ENCFF926NKP", "ENCFF747IZX", "ENCFF893ASJ", "ENCFF712PWV", "ENCFF763UAG",
    "ENCFF500XXC", "ENCFF553MQN", "ENCFF806JEJ", "ENCFF303KLN", "ENCFF610UAL",
    "ENCFF979MUT"
]

# Function to generate example matrices for cell lines
def generate_example_matrix():
    return np.random.rand(len(common_names), 200)  # 21 cell lines, 200 positions

# Function to construct a similarity network from a matrix
def construct_similarity_network(matrix):
    similarity_matrix = cosine_similarity(matrix)
    edge_index = torch.tensor(np.nonzero(np.triu(similarity_matrix)), dtype=torch.long)
    edge_weight = torch.tensor(similarity_matrix[np.nonzero(np.triu(similarity_matrix))], dtype=torch.float)
    num_nodes = matrix.shape[0]
    x = torch.eye(num_nodes)
    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight)

# Define a simple GCN model
class GCN(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Function to train and predict with GCN
def train_and_predict_with_gcn(data, num_features, num_classes):
    model = GCN(num_features=num_features, num_classes=num_classes)
    # Dummy prediction for example purposes
    return torch.rand((data.x.size(0), num_classes))

# Define a simple neural network for regression
class RegressionNN(nn.Module):
    def __init__(self, input_size, output_size):
        super(RegressionNN, self).__init__()
        self.layer1 = nn.Linear(input_size, 128)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(128, output_size)

    def forward(self, x):
        out = self.layer1(x)
        out = self.relu(out)
        out = self.layer2(out)
        return out

# Example function to generate and process data, then train the model
def run_example():
    # Generate example data
    tf_matrices = [generate_example_matrix() for _ in range(3)]
    h3k27ac_matrices = [generate_example_matrix() for _ in range(3)]
    
    # Construct similarity networks
    tf_networks = [construct_similarity_network(matrix) for matrix in tf_matrices]
    h3k27ac_networks = [construct_similarity_network(matrix) for matrix in h3k27ac_matrices]
    
    # Get predictions from GCN
    tf_prediction = train_and_predict_with_gcn(tf_networks[0], num_features=200, num_classes=10)
    h3k27ac_prediction = train_and_predict_with_gcn(h3k27ac_networks[0], num_features=200, num_classes=10)
    
    # Combine predictions
    combined_predictions = torch.cat([tf_prediction, h3k27ac_prediction], dim=1)
    
    # Convert to numpy for regression
    combined_predictions_np = combined_predictions.detach().numpy()
    
    # Dummy target values (replace with actual data)
    Y = np.random.rand(combined_predictions_np.shape[0])
    
    # Train linear regression model
    linear_model = LinearRegression()
    linear_model.fit(combined_predictions_np, Y)
    linear_prediction = linear_model.predict(combined_predictions_np)
    
    # Train neural network for regression
    input_size = combined_predictions_np.shape[1]
    output_size = 1
    regression_model = RegressionNN(input_size, output_size)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(regression_model.parameters(), lr=0.01)
    
    # Convert Y to tensor
    Y_tensor = torch.tensor(Y, dtype=torch.float32).view(-1, 1)
    combined_predictions_tensor = torch.tensor(combined_predictions_np, dtype=torch.float32)
    
    # Training loop
    epochs = 100
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = regression_model(combined_predictions_tensor)
        loss = criterion(outputs, Y_tensor)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}')
    
    # Make final predictions
    final_prediction = regression_model(combined_predictions_tensor).detach().numpy()
    
    # Visualization
    plt.scatter(Y, final_prediction, alpha=0.5)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs. Predicted Values')
    plt.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=4)
    plt.show()

# Run the example
run_example()
